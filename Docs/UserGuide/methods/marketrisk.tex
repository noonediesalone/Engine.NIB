\chapter{Market Risk}

\section{Sensitivity Analysis}\label{sec:app_sensi}

ORE's sensitivity analysis framework uses ``bump and revalue'' to compute Interest Rate, FX, Inflation, Equity and Credit sensitivities to
\begin{itemize}
\item Discount curves  (in the zero rate domain)
\item Index curves (in the zero rate domain)
\item Yield curves including e.g. equity forecast yield curves (in the zero rate domain)
\item FX Spots
\item FX volatilities
\item Swaption volatilities, ATM matrix or cube 
\item Cap/Floor volatility matrices (in the caplet/floorlet domain)
\item Default probability curves (in the ``zero rate'' domain, expressing survival probabilities $S(t)$ in term of zero rates $z(t)$ via $S(t)=\exp(-z(t)\times t)$ with Actual/365 day counter)
\item Equity spot prices
\item Equity volatilities, ATM or including strike dimension 
\item Zero inflation curves
\item Year-on-Year inflation curves
\item CDS volatilities
\item Base correlation curves
\end{itemize}

Apart from first order sensitivities (deltas), ORE computes second order sensitivities (gammas and cross gammas) as well. Deltas are computed using up-shifts and base values as
$$
\delta = \frac{f(x+\Delta)-f(x)}{\Delta},
$$ 
where the shift $\Delta$ can be absolute or expressed as a relative move $\Delta_r$ from the current level, $\Delta=x\,\Delta_r$. Gammas are computed using up- and down-shifts
$$
\gamma = \frac{f(x+\Delta)+f(x-\Delta) - 2\,f(x)}{\Delta^2},
$$ 
cross gammas using up-shifts and base values as
$$
\gamma_{cross} = \frac{f(x+\Delta_x,y+\Delta_y)-f(x+\Delta_x,y) -f(x,y+\Delta_y) + f(x,y)}{\Delta_x\,\Delta_y}.
$$ 

From the above it is clear that this involves the application of 1-d shifts (e.g. to discount zero curves) and 2-d shifts (e.g. to Swaption volatility matrices). The structure of the shift curves/matrices does not have to match the structure of the underlying data to be shifted, in particular the shift ``curves/matrices'' can be less granular than the market to be shifted. 
Figure \ref{fig_shiftcurve} illustrates for the one-dimensional case how shifts are applied.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.6]{shiftcurve.pdf}
\end{center}
\caption{1-d shift curve (bottom) applied to a more granular underlying curve (top). }
\label{fig_shiftcurve}
\end{figure} 

Shifts at the left and right end of the shift curve are extrapolated flat, i.e. applied to all data of the original curve to the left and to the right of the shift curve ends. In between, all shifts are distributed linearly as indicated to the left and right up to the adjacent shift grid points. As a result, a parallel shift of the all points on the shift curve yields a parallel shift of all points on the underlying curve.   \\

The two-dimensional case is covered in an analogous way, applying flat extrapolation at the boundaries and ``pyramidal-shaped'' linear interpolation for the bulk of the points. 

The details of the computation of sensitivities to implied volatilities in strike direction can be summarised as
follows, see also table \ref{sensi_config_overview} for an overview of the admissible configurations and the results
that are obtained using them.

\medskip
For {\em Swaption Volatilities}, the initial market setup can be an ATM surface only or a full cube. The simulation
market can be set up to simulate ATM only or to simulate the full cube, but the latter choice is only possible if a full cube is set
up in the initial market. The sensitivity set up must match the simulation setup with regards to the strikes (i.e. it
is ATM only if and only if the simulation setup is ATM only, or it must contain exactly the same strike spreads relative
to ATM as the simulation setup). Finally, if the initial market setup is a full cube, and the simulation / sensitivity
setup is to simulate ATM only, then sensitivities are computed by shifting the ATM volatility w.r.t. the given shift size and type and
shifting the non-ATM volatilities by the same absolute amount as the ATM volatility.

\medskip
For {\em Cap/Floor Volatilities}, the initial market setup always contains a set of fixed strikes, i.e. there is no
distinction between ATM only and a full surface. The same holds for the simulation market setup. The sensitivity setup
may contain a different strike grid in this case than the simulation market. Sensitivity are computed per expiry and
per strike in every case.

\medskip
For {\em Equity Volatilities}, the initial market setup can be an ATM curve or a full surface. The simulation market can
be set up to simulate ATM only or to simulate the full surface, where a full surface is allowed even if the initial market setup in an
ATM curve only. If we have a full surface in the initial market and simulate the ATM curve only in the simulation market, sensitivities
are computed as in the case of Swaption Volatilities, i.e. the ATM volatility is shifted w.r.t. the specified shift size
and type and the non-ATM volatilities are shifted by the same absolute amount as the ATM volatility. If the simulation
market is set up to simulate the full surface, then all volatilities are shifted individually using the specified shift size and type. In
every case the sensitivities are aggregated on the ATM bucket in the sensitivity report.

\medskip
For {\em FX Volatilities}, the treatment is similar to Equity Volatilities, except for the case of a full surface
definition in the initial market and an ATM only curve in the simulation market. In this case, the pricing in the
simulation market is using the ATM curve only, i.e. the initial market's smile structure is lost.

\medskip
For {\em CDS Volatilities} only an ATM curve can be defined.

\medskip
In all cases the smile dynamics is ``sticky strike'', i.e. the implied vol used for pricing a deal does not change if
the underlying spot price changes.

\begin{table}[hbt]
  \scriptsize
  \begin{center}
    \begin{tabular}{l | l | l | l | l | l}
      \hline
      Type & Init Mkt. Config. & Sim. Mkt Config. & Sensitivity Config. & Pricing & Sensitivities w.r.t. \\
      \hline
      Swaption & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      Swaption & Cube & Simulate Cube & Shift Smile Strikes & Full Cube & Smile Strike Shifts\footnote{smile
                                                                          strike spreads must match simulation market configuration} \\
      Swaption & Cube & Simulate ATM only & Shift ATM only & Full Cube & ATM Shifts\footnote{smile is shifted in parallel\label{sensismileparallel}} \\
      \hline
      Cap/Floor & Surface & Simulate Surface & Shift Smile Strikes & Full Surface & Smile Strike Shifts \\
      \hline
      Equity & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      Equity & ATM & Simulate Surface & Shift ATM only & ATM Curve & Smile Strike Shifts\footnote{result sensitivities
                                                                     are aggregated on ATM\label{sensiaggatm}} \\
      Equity & Surface & Simulate ATM only & Shift ATM only & Full Surface & ATM Shifts\textsuperscript{\ref{sensismileparallel}} \\
      Equity & Surface & Simulate Surface & Shift ATM only & Full Surface & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      \hline
      FX & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      FX & ATM & Simulate Surface & Shift ATM only & ATM Curve & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      FX & Surface & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
      FX & Surface & Simulate Surface & Shift ATM only & Full Surface & Smile Strike Shifts\textsuperscript{\ref{sensiaggatm}} \\
      \hline
      CDS & ATM & Simulate ATM only & Shift ATM only & ATM Curve & ATM Shifts \\
    \end{tabular}
    \caption{Admissible configurations for Sensitivity computation in ORE}
    \label{sensi_config_overview}
  \end{center}
  \end{table}

\section{Par Sensitivity Analysis}
\label{app:par_sensi}

The ``raw'' sensitivities in ORE are generated in a computationally convenient domain (such as zero rates, caplet/floorlet volatilities, integrated hazard rates, inflation zero rates). These raw sensitivities are typically further processed in risk analytics such as VaR measures. On the other hand, for hedging purposes one is rather interested in sensitivities with respect to fair rates of hedge instruments such as Forward Rate Agreements, Swaps, flat Caps/Floors, CDS, Zero Coupon Inflation Swaps. \\

It is possible to generate par sensitivities from raw sensitivities using the chain rule as follows, and this is the approach taken in ORE. Recall for example the fair swap rate $c$ for some maturity as a function of zero rates $z_i$ in a single curve setting:
$$
c = \frac{1 - e^{-z_n\,t_n}}{\sum_{i=1}^n \delta_i\,e^{-z_i\, t_i}}
$$
More realistically, a given fair swap rate might be a function of the zero rates spanning the discount and index curves in the chosen currency. In a multi currency curve setting, that swap rate might even be a function of the zero rates spanning a foreign (collateral) currency discount curve, foreign and domestic currency index curves. Generally, we can write any fair par rate $c_i$ as function of raw rates $z_j$,
$$
c_i \equiv c_i(z_1, z_2, ..., z_n)
$$
This function may not be available in closed form, but numerically we can evaluate the sensitivity of $c_i$ with respect to changes in all raw rates,
$$
\frac{\partial c_i}{\partial z_j}.
$$
These sensitivities form a {\em Jacobi} matrix of derivatives. Now let $V$ denote some trade's price. Its sensitivity with respect a raw rate change $\partial V/\partial z_k$ can then be expressed in terms of sensitivities w.r.t. par rates using the chain rule
$$
\frac{\partial V}{\partial z_j} = \sum_{i=1}^n \frac{\partial V}{\partial c_i}\,\frac{\partial c_i}{\partial z_j},
$$
or in vector/matrix form
$$
\nabla_z V = C \cdot \nabla_c V, \qquad C_{ji} = \frac{\partial c_i}{\partial z_j}.
$$
Given the raw sensitivity vector $\nabla_z V$, we need to invert the Jacobi matrix $C$ to obtain the par rate sensitivity vector
$$
\nabla_c V = C^{-1} \cdot \nabla_z V.
$$

We then compute the Jacobi matrix $C$ by
\begin{itemize}
\item setting up par instruments with links to all required term structures expressed in terms of raw rates
\item ``bumping'' all relevant raw rates and numerically computing the par instrument's fair rate shift for each bump
\item thus filling the Jacobi matrix with finite difference approximations of the partial derivatives $\partial c_i/\partial z_j$.
\end{itemize}

The par rate conversion supports the following par instruments:
\begin{itemize}
\item Deposits
\item Forward rate Agreements
\item Interest Rate Swaps (fixed vs. ibor)
\item Overnight Index Swaps
\item Tenor Basis Swaps (ibor vs. ibor)
\item Overnight Index Basis Swaps (ibor vs. OIS)
\item FX Forwards
\item Cross Currency Basis Swaps
\item Credit Default Swaps
\item Caps/Floors
\end{itemize}


\section{Economic P\&L}\label{economic_pnl}

The economic P\&L of a portfolio denotes the change in its economic value over  
a time period $t_1$ to $t_2$. The economic value evolution during the period is due to three components
 
\begin{itemize}
\item the change in present value from period start to end
\item incoming and outgoing cash flows
\item accumulated cost of funding required to set up the portfolio initially 
\end{itemize}

In the following, we consider a portfolio consisting of assets in various currencies. We decompose the portfolio into parts each denominated in a different currency and value each sub-portfolio in its currency. We denote the sub-portfolio values at time $t$ in the respective currency $P_1(t), P_2(t), \dots$. Instruments with cash flows in more than one currency are decomposed into single-currency instruments and assigned into the related sub-portfolio.
The total portfolio value expressed in base currency (e.g. EUR) is 
\begin{equation} 
	P(t) = \sum_c P_c(t)\:X_c(t) \label{initial_value_base}
\end{equation}
where $X_c$ is the exchange rate that converts an amount in currency $c$ into an amount in base currency by multiplication. All prices $P_c(t)$ denote {\em dirty} market values (or theoretical values where market values are not available) at time $t$. 

In the following we consider three points in time,
\begin{itemize}
\item $t_0$: the time just before the first actual cash flow has appeared in the portfolio under consideration, possibly years ago
\item $t_1$: the beginning of the period for which we want to determine P\&L
\item $t_2$: the end of the period for which we want to determine P\&L
\end{itemize} 

\subsection*{Original P\&L} \label{trade_start}

The original P\&L is the portfolio's P\&L from portfolio inception $t_0$.
In this case the portfolio value at $t_0$ is 
$$P(t_0) = 0,$$
and the P\&L up to time $t_2$ is given by the portfolio value at $t_2$ plus the balance of currency accounts that collect incoming and outgoing cash flows and are compounded up to time $t_2$:
\begin{equation}
\pi(0, t_2) = P(t_2) + \sum_c X_c(t_2)\:B_c(t_2)
\label{pnl_3}
\end{equation}
where 
\begin{equation}
B_c(t_2) = \sum_{j=0}^{I(t_2)-1} F_c(\tau_j)\:C_c(\tau_j, t_2), \quad C_c(\tau_j, t_2)=\prod_{k=I(\tau_j)}^{I(t_2)-1} (1+r_c(\tau_k)\delta_k),
\end{equation}
sums and products are taken over daily time steps $\tau_j$ and

\medskip
\begin{tabular}{lp{10cm}}
$I(t)$ & is the day's index associated with time $t$ \\
$F_c(\tau_{j})$ & is the net cash flow in currency $c$ on date/time $\tau_j$, possibly zero \\
$r_c(\tau_j)$ & is the Bank's overnight funding and investment rate in currency $c$ for interest period $[\tau_j, \,\tau_{j+1}]$ (overnight)\\
$\delta_j$ & is the related day count fraction for period $[\tau_j, \,\tau_{j+1}]$ 
\end{tabular}

\bigskip

The balances $B_c$ can also be constructed iteratively
\begin{eqnarray*}
B_c(\tau_{j+1}) &=& B_c(\tau_{j}) (1+r_c(\tau_j)\delta_j) + F_c(\tau_{j+1}) 
\label{recursion}\\
j &=& 0, 1, 2, \dots \nonumber\\
B_c(\tau_0) &=& 0. \nonumber
\end{eqnarray*}

The P\&L for a period of interest $[t_1;\,t_2]$ is then computed by taking the difference
\begin{eqnarray}
	\pi(t_1, t_2) &=& \pi(0,t_2) - \pi(0,t_1) \label{pnl_1} \\
		&=& P(t_2) - P(t_1) + \sum_c (X_c(t_2)\: B_c(t_2) - X_c(t_1)\: B_c(t_1))
		\nonumber
\end{eqnarray}

One can show that
\begin{equation} 
	B_c(t_2) = B_c(t_1) \:C_c(t_1, t_2) 
	+ \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2) 
\label{bc}
\end{equation}
which separates the contribution to $B_c(t_2)$ from cash flows in period $[t_1;t_2]$ (right-most sum) and contributions from realized P\&L and cost of funding of previous periods accumulated in $B_c(t_1)$. We can now insert (\ref{bc}) into (\ref{pnl_1}) to eliminate $B_c(t_2)$ and obtain 
\begin{eqnarray}
	\pi(t_1, t_2) &=& P(t_2) - P(t_1)
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& + \sum_c B_c(t_1)\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\} \label{pnl_1a}
\end{eqnarray}
 
\subsection*{Cost of Carry}

Separating actual cash flows and prices from compounding effects yields
$$
	\pi(t_1, t_2) = P(t_2) - P(t_1)
	+ \sum_c X_c(t_2)\sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)
	+ CC(t_1,t_2)
$$

where the cost of carry term is 
\begin{eqnarray}
CC(t_1,t_2) &=& 
	\sum_c X_c(t_2)\sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:
	\left(C_c(\tau_j, t_2) - 1\right) \nonumber\\
&& + \sum_c \:B_c(t_1)\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\} 
\label{CC}
\end{eqnarray}

\subsection*{Period P\&L after Sell-Down}

At time $t_1$, we can write the original P\&L (equation \ref{pnl_3}) in respective currencies
$$
\pi_c(t_1) = P_c(t_1) + B_c(t_1), \qquad \pi(t_1) = \sum_c X_c(t_1)\:\pi_c(t_1).
$$
Inserting this into (\ref{CC}),
\begin{eqnarray*}
CC(t_1,t_2) &=& 
	\sum_c X_c(t_2)\sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:
	\left(C_c(\tau_j, t_2) - 1\right) \\
&& + \sum_c \:(\pi_c(t_1) - P_c(t_1))\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\}, 
\end{eqnarray*}
shows that there is a contribution to $\pi(t_1,t_2)$, via the cost of carry, due to compounding and FX effects on previous periods' P\&L result.

\bigskip
We now take the view that the portfolio is liquidated at time $t_1$, so that the account balance equals the P\&L at $t_1$. We further assume that this balance is then removed ("sell down" of P\&L) and transfered into a separate portfolio, the Bank's equity\footnote{Equity is in turn managed and most likely invested into financial instruments other than a Bank account}. The same portfolio is thereafter set up again so that the currency account balance turns into a liability $B_c(t_1) = - P_c(t_1)$, and the total starting balance is $B(t_1)=-P(t_1)$. In contrast to the previous section, this changes the balance at time $t_1$ suddenly and without relation to an actual cash flow.

This raises the question how the artificial initial balance is funded subsequently, in currency for each sub-portfolio or in base currency only. This
choice may vary by portfolio, depend on the actual currencies in which the Bank can source funding, depend on the location/economy in which the portfolio is run, which currency is a reasonable benchmark, etc. 

\subsection*{Funding in Currency}\label{funding_ccy}

In this section we take the view that each sub-portfolio is funded in currency
so that we start with opening balances $B_c(t_1) = -P_c(t_1)$.

Inserting the artificial opening balances at $t_1$ into (\ref{pnl_1a}) yields
\begin{eqnarray}
	\pi_2(t_1, t_2) &=& P(t_2) - P(t_1)
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& - \sum_c P_c(t_1)\:\left\{X_c(t_2)\:C_c(t_1, t_2) - X_c(t_1)\right\} \nonumber \\
&=&	P(t_2)
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& - \sum_c P_c(t_1)\:X_c(t_2)\:C_c(t_1, t_2)
	\label{pnl_2}
\end{eqnarray}

Note that only exchange rates at $t_2$ enter into the expression.

\subsection*{Cost of Carry}

Separating actual cash flows and prices from compounding effects yields

$$
	\pi_2(t_1, t_2) = P(t_2) + \sum_c X_c(t_2)\:\left\{-P_c(t_1)
	+ \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\right\}
	+ CC_2(t_1,t_2)
$$

where the cost of carry term is 
\begin{eqnarray*}
CC_2(t_1,t_2) &=& \sum_c \:X_c(t_2)\:
\sum_{j=I(t_1)}^{I(t_2)-1} F_c(\tau_j) \: \left(C_c(\tau_j, t_2) - 1\right)\\
&& - \sum_c \:X_c(t_2)\:P_c(t_1)\:\left(C_c(t_1, t_2) - 1\right)
\end{eqnarray*}

\subsection*{Funding in Base Currency}\label{funding_base_ccy}

In this section we assume that the setup cost for the portfolio is converted into base currency at $t_1$ and funded subsequently in base currency.
This means we insert artificial initial balances $B_c(t_1)=0$ except for the base currency account $B(t_1) = -P(t_1)$.
Inserting this opening balance at $t_1$ into (\ref{pnl_1a}) now yields
\begin{eqnarray}
	\pi_3(t_1, t_2) &=& P(t_2) 
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:C_c(\tau_j, t_2)
	\nonumber\\
	&& - P(t_1)\:C(t_1, t_2)
	\label{pnl_4}
\end{eqnarray}

where $C(t_1,t_2)$ is the compounding factor in base currency.

\subsubsection*{Cost of Carry}

Separating actual cash flows and prices from compounding effects yields now

$$
	\pi_3(t_1, t_2) = P(t_2) - P(t_1) 
	+ \sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)
	+ CC_3(t_1, t_2)
$$
where
\begin{eqnarray*}
CC_3(t_1, t_2) &=& 
\sum_c X_c(t_2) \sum_{j=I(t_1)}^{I(t_2)-1}F_c(\tau_j)\:(C_c(\tau_j, t_2)-1)\\
&&  - P(t_1)\:(C(t_1, t_2) - 1)
\end{eqnarray*}

\subsubsection*{FX Effect}\label{fx_effect}

The difference between (\ref{pnl_2}) and (\ref{pnl_3}) is 

$$ \pi_2(t_1, t_2) - \pi_3(t_1, t_2) = P(t_1)\:C(t_1, t_2) 
	- \sum_c P_c(t_1)\:X_c(t_2)\:C_c(t_1, t_2).
$$

The expected value of this difference at period start $t_1$ is zero, but the retrospectively realized difference at period end $t_2$ is nonzero in general.
 

\section{Risk Hypothetical P\&L}\label{sec:pl}

In the following we briefly describe approaches to generating P\&L
vectors that feed into several subsequent sections for the purpose of
computing risk measures such as Value at Risk or for backtesting a
market risk model.

These P\&L's are different from the economic P\&L introduced in
section \ref{economic_pnl} above, but rather {\em risk-hypothetical} 
due to the application  of some historical market moves to the
current market which gives rise to a valuation change.
  
%\subsection{Basics}\label{sec:pl_basics}

Consider a history of market risk factors $X_i(j)$ where $i\in\{1,\ldots,n\}$ identifies the risk factor and
$j\in\{1,\ldots,m\}$ corresponds to a time $t_j$ on which the risk factor was observed. The times are assumed to be
equally spaced, with $t_{j+1}-t_j$ corresponding to $1$ business day w.r.t. a given calendar. To generate an overlapping
$k$-day PL, define the $k$ day return at time $t_j$ to be

\begin{equation}\label{returns}
r_i(j) = R_{T_i}(X_i(j), X_{i}(j+k))
\end{equation}

for $j=1,\ldots,m-k$. Note that the case of non-overlapping $k$ day returns fits in with straightforward modifications
of the scheme described here. $R$ defines the return value for two observations of the same factor, which is one of the
following

\begin{eqnarray}
  R_A(x,y) &=& y-x \\
  R_R(x,y) &=& y/x - 1 \\
  R_L(x,y) &=& \log(y/x)
\end{eqnarray}

where the subscript stands for absolute (A), relative (R) and lognormal (L) returns, respectively. Note that the
relative and lognormal returns are not defined for $x=0$, and we consider a data point with $X_i(j)=0$ and for which we
compute relative or lognormal returns to be an error in the data that needs to be corrected or excluded from the
analysis. Also note that $R_R \approx R_L$ for small values of $y/x-1$, the difference $R_R-R_L$ approaching zero when
$y/x$ approaches $1$.

Now assume $t_m$ to be the reference date (e.g. for the value at risk calculation) and

\begin{equation}
  X(m) = \{ X_i(m) \}_{i=1,\ldots,n}
\end{equation}

the market factor values on the reference date.

\subsection*{Full Revaluation P\&L}\label{fullpl}

For a given portfolio denote its NPV at $t_m$ by $\nu(X(m))$. Then we can compute a {\em full revaluation PL} vector

\begin{equation}\label{fullrevalpl}
\pi_F = \{ \pi_F(j) \}_{j=1,\ldots,m-k}
\end{equation}

as

\begin{equation}
  \pi_F(j) = \nu( X'(m,j) ) - \nu ( X(m) )
\end{equation}

by pricing the portfolio under each perturbed market factor vector

\begin{equation}
  X'(m,j) = \{ X'_i(m,j) \}_{i=1,\ldots,n}
\end{equation}

which is defined by

\begin{equation}\label{histReturns}
  X'_i(m,j) = a_{T_i}( X_i(m),  R_{T_i}(X_i(j),X_i(j+k)) )
\end{equation}

with the return application function

\begin{eqnarray}
  a_A(x,r) &=& x+r \\
  a_R(x,r) &=& x(1+r) \\
  a_L(x,r) &=& x e^r
\end{eqnarray}

and return types $T_i \in \{ A, R, L \}$, dependent on the particular factor $X_i$. Table \ref{sensiReturnTypes} shows a
possible choice of return types for the different risk factors (in ORE notation). Note, that the factors Discount
Curve, Index Curve and Survival Probability are discount factors resp. survival probabilities that are converted to zero
rate resp. hazard rate shifts by taking the log. Also note, that the factors Recovery Rate and Basis Correlation are
bounded (a recovery rate must be in $[0,1]$ while the base correlation must be in $[-1,1]$), so that after a shift is
applied, the result has to be capped / floored appropriately to ensure valid scenario values.

\subsection*{Sensitivity based P\&L}\label{sensipl}

As an alternative to the full revaluation PL in \eqref{fullrevalpl} we can approximate this PL using a Taylor expansion
of $\nu(X'(m,j))$ viewed as a function of the returns $R_{T_i}$\footnote{i.e. we view $\nu$ as a function of the second
argument of $a_{T_i}$ in \ref{histReturns}} around the expansion point $(0,0,\ldots,0)$ generating a {\em sensitivity
  based PL},

\begin{equation}
\pi_S = \{ \pi_S(j) \}_{j=1,\ldots,m-k}
\end{equation}

with

\begin{equation}
\begin{aligned}\label{taylorPl}
  \pi_S(j) = & \sum_{i=1}^n D^i_{T_i}\nu(X(m)) R_{T_i}(X_i(m), X'_i(m,j)) + \\
           \frac{1}{2}& \sum_{i,l=1}^n D^{i,l}_{T_i,T_l}\nu(X(m)) R_{T_i}(X_i(m), X'_i(m,j)) R_{T_l}(X_l(m), X'_l(m,j)),
\end{aligned}
\end{equation}

where we use sensitivities up to second order. Here $D^i_{T_i}$ denotes a first or second order derivative operator, depending
on the market factor specific shift type $T_i \in \{ A,R,L \}$, i.e.

\begin{eqnarray}\label{derivs}
  D^i_A f(x) &=& \frac{\partial f(x)}{\partial x_i}, \\
  D^i_R f(x) = D^i_L f(x) &=& x_i\frac{\partial f(x)}{\partial x_i}
\end{eqnarray}

and using the short hand notation

\begin{equation}\label{derivs_short}
  D^{i,l}_{T_i,T_l} f(x) = D^i_{T_i} D^l_{T_l} f(x).
\end{equation}

These first and second order sensitivities may be computed analytically, or (more common) as finite difference
approximations (``bump and revalue'' approximations), see section \ref{sec:app_sensi}. To clarify the relationship of \eqref{derivs}
and a finite difference scheme for derivatives computation in a bit more detail we note that for a absolute shift $h>0$

\begin{equation}
\frac{f(x+h)-f(x)}{h} \rightarrow f'(x)
\end{equation}

for $h\rightarrow 0$ by definition of $f'$ while for a relative shift

\begin{equation}
  \frac{f(x(1+h))-f(x)}{h} = x \frac{f(x(1+h))-f(x)}{xh} \rightarrow xf'(x)
\end{equation}

for $h\rightarrow 0$ and for a log shift

\begin{equation}\label{logshift}
\frac{f(xe^h)-f(x)}{h} \rightarrow xf'(x)
\end{equation}

using e.g. L'Hospital's rule, so that

\begin{itemize}
\item both a relative and a log shift bump and revalue sensitivity approximate the same value $xf'(x)$ in the limit for
  $h\rightarrow 0$,
\item an absolute shift sensitivity can be transformed into a relative / log shift sensitivity (in the limit for
  $h\rightarrow 0$) by multiplying with the risk factor value $x$, and vice versa.
\end{itemize}

We also note that the usual way of bumping continuously compounded zero rates to compute a Discount Curve or Index Curve
sensitivity by $h^*$ is equivalent to \eqref{logshift} with $h=h^*t$, where $t$ is the maturity of the respective
rate. Therefore in practice a log return of discount factors can not directly be combined with a sensitivity expressed
in zero rate shifts, but has to be scaled by $1/t$ before doing so.

Since the number of second order derivatives can be quite big in realistic setups with hundreds or even thousands of
market factors, in practice only part of the second order derivatives might be fed into \eqref{taylorPl} assuming the
rest to be zero.

Note that the types $T_i$ used to generate the historical returns \eqref{histReturns} can be different from those used
in the Taylor expansion \eqref{taylorPl}. It is important though that the same types $T_i$ are used for the
derivatives operators $D^i_{T_i}$ and the returns $R_{T_i}$ in \eqref{taylorPl}. 

A number of configurations are hard-coded into ORE depending on whether raw sensitivities, backtesting sensitivities or CRIF sensitivities are being called. These configurations are displayed in \ref{sensiReturnTypes} - note that there is currently no distinction made in ORE between raw sensitivities and backtest sensitivities.

\begin{table}[ht]
  \begin{tabular}{|l|c|c|c|c|}
    \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{ORE Risk Factor}} & \multicolumn{2}{c|}{Backtest Sensitivities} & \multicolumn{2}{c|}{CRIF Sensitivities} \\ \cline{2-5} 
    \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Return Type} & \multicolumn{1}{c|}{Shift Size} & \multicolumn{1}{c|}{Return Type} & \multicolumn{1}{c|}{Shift Size} \\ \hline
    Discount Curve & A & 0.01\% & A & 0.01\% \\
    Index Curve & A & 0.01\% & A & 0.01\% \\
    Yield Curve & A & 0.01\% & A & 0.01\% \\
    Dividend Yield & A & 0.01\% & A & 0.01\% \\
    Equity Forecast Curve & A & 0.01\% & A & 0.01\% \\
    Swaption Volatility* & R & 1\% & A & 0.01\% \\
    Optionlet Volatility* & R & 1\% & A & 0.01\% \\
    FX Spot** & R & 1\% & R & 0.1\% \\
    FX Volatility & R & 1\% & A & 1\% \\
    Equity Spot & R & 1\% & R & 1\% \\
    Equity Volatility & R & 1\% & A & 1\% \\
    Yield Volatility & R & 1\% & R & 1\% \\
    Survival Probability & A & 0.01\% & A & 0.01\% \\
    CDS Volatility & R & 1\% & R & 1\% \\
    Correlation & R & 1\% & - & - \\
    Base Correlation & A & 1\% & A & 1\% \\
    Zero Inflation Curve & A & 0.01\% & A & 0.01\% \\
    YoY Inflation Curve & A & 0.01\% & A & 0.01\% \\
    Zero Inflation CF Vol & R & 1\% & R & 1\% \\
    YoY Inflation CF Vol & R & 1\% & R & 1\% \\
    Commodity Curve & R & 1\% & R & 1\% \\
    Commodity Volatility & R & 1\% & A & 1\% \\
    Security Spread & A & 0.01\% & - & - \\ \hline
  \end{tabular}
  \caption{Sensitivity return type configuration for raw, backtest and CRIF sensitivities}
  \label{sensiReturnTypes}
\end{table}

\newpage
*We predominantly use normal IR volatilities with an absolute shift of $0.01\%$. For lognormal IR volatilities an absolute shift of $1\%$ applies. Also, notice that ``Optionlet Volatility'' is the ORE name for ``Cap / Floor Volatility''.

**During a CRIF run, the FX spot delta is computed using the central difference approximation by default. A smaller shift size of $0.1\%$ is used because of this. Furthermore, for FX vanilla European and American options, there are analytic formulae available for FX deltas. These are implemented in ORE during a CRIF run.

Note - the values $A$ and $R$ here refer to the absolute and relative shifts defined in \eqref{derivs}. 

\section{Value at Risk}\label{sec:app_var}

\subsection{Historical Simulation VaR}\label{histsimvar}

The historical simulation VaR is defined to be a $p$-quantile of the empirical distribution generated by the full
revaluation PL vector $\pi_F = \{ \pi_F(j) \}_{j}$. Here, with ``generated'' we mean that we weigh each $\pi_F(j)$ with
the same probability $1/J$, where $J$ denotes the number of elements in the vector.

\subsection{Historical Simulation Taylor VaR}\label{histtaylorvar}

Similarly, the historical simulation Taylor VaR is defined to be the $p$-quantile of the empirical distribution
generated by the sensitivity based PL vector $\pi_S$ (call side), resp. $-\pi_S$ (post side).

\subsection{Parametric VaR}\label{parametricvar}

For the computation of the parametric, or variance-covariance VaR, we rely on a second order sensitivity-based P\&L approximation

\begin{eqnarray}\label{taylorPl2}
  \pi_S & = & \sum_{i=1}^n D^i_{T_i}\,V\cdot Y_i 
        + \frac{1}{2} \sum_{i,j=1}^n D^{i,j}_{T_i,T_j}\,V\cdot Y_i\cdot Y_j
\end{eqnarray}

with 
\begin{itemize}
\item portfolio value $V$
\item random variables $Y_i$ representing risk factor returns; these are assumed to be multivariate normally distributed with zero mean
and covariance matrix matrix $C = \{ \rho_{i,k} \sigma_i \sigma_k \}_{i,k}$, where $\sigma_i$ denotes the standard
deviation of $Y_i$; covariance matrix $C$ may be estimated using the Pearson estimator on historical return data
$\{ r_i(j) \}_{i,j}$. Since the raw estimate might not be positive semidefinite, we apply a salvaging algorithm to
ensure this property, which basically replaces negative Eigenvalues by zero and renormalises the resulting matrix, see
\cite{corrSalv};
\item first or second order derivative operators $D$, depending
on the market factor specific shift type $T_i \in \{ A,R,L \}$ (absolute shifts, relative shifts, absolute log-shifts), i.e.
\begin{eqnarray*}\label{derivs2}
  D^i_A \,V(x) &=& \frac{\partial V(x)}{\partial x_i} \\
  D^i_R \,V(x) = D^i_L f(x) &=& x_i\frac{\partial V(x)}{\partial x_i}
\end{eqnarray*}
and using the short hand notation
\begin{equation*}
  D^{i,j}_{T_i,T_j} V(x) = D^i_{T_i} D^j_{T_j} V(x)
\end{equation*}
In ORE, these first and second order sensitivities are computed as finite difference
approximations (``bump and revalue'').
\end{itemize}

To approximate the $p$-quantile of $\pi_S$ in \eqref{taylorPl2} ORE offers the techniques outlined below.

\subsection{Delta Gamma Normal Approximation}
 
The distribution of \eqref{taylorPl2} is non-normal due to the second order terms. 
The delta gamma normal approximation in ORE computes mean $m$ and variance $v$ of the portfolio value change $\pi_S$ (discarding moments higher than two) following \cite{alexander} and provides a simple VaR estimate 
$$
VaR = m + N^{-1}(q)\,\sqrt{v}
$$
for the desired quantile $q$ ($N$ is the cumulative standard normal distribution). Omitting the second order terms in \eqref{taylorPl2} yields the delta normal approximation.
 
\subsection{Cornish-Fisher Expansion}

The first four moments of the distribution of $\pi_S$ in \eqref{taylorPl2} can be computed in closed form using the covariance matrix $C$ and the sensitivities of first and second order $D_i$
and $D_{i,k}$, see e.g. \cite{alexander}. Once these moments are known, an approximation to the true quantile of $\pi_S$ can be computed using the Cornish-Fisher expansion, see also [7], which in practice often gives a decent approximation of the true value, but may also show bigger differences in certain configurations.

\subsection{Saddlepoint Approximation}

Another approximation of the true quantile of $\pi_S$ can be computed using the Saddlepoint approximation using results from \cite{Lugannani} and \cite{Daniels}. This method typically produces more accurate results than the Cornish-Fisher method, while still being fast to evaluate.

\subsection{Monte Carlo Simulation}

By simulating a large number of realisations of the return vector $Y=\{ Y_i \}_i$ and computing the corresponding
realisations of $\pi_S$ in \eqref{taylorPl2} we can estimate the desired quantile as the quantile of the empirical
distribution generated by the Monte Carlo samples. Apart from the Monte Carlo Error no approximation is involved in this
method, so that albeit slow it is well suited to produce values against which any other approximate approaches can be tested. Numerically, the simulation is implemented using a Cholesky Decomposition
of the covariance matrix $C$ in conjunction with a pseudo random number generator (Mersenne Twister) and an
implementation of the inverse cumulative normal distribution to transform $U[0,1]$ variates to $N(0,1)$ variates.

